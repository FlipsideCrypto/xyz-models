# Name your project! Project names should contain only lowercase characters
# and underscores. A good package name should reflect your organization's
# name or the intended use of these models
name: "datascience_models"
version: "1.0.0"
config-version: 2

require-dbt-version: ">=1.4.0"

# This setting configures which "profile" dbt uses for this project.
profile: "datascience"

# These configurations specify where dbt should look for different types of files.
# The `model-paths` config, for example, states that models in this project can be
# found in the "models/" directory. You probably won't need to change these!
model-paths: ["models"]
analysis-paths: ["analysis"]
test-paths: ["tests"]
seed-paths: ["data"]
macro-paths: ["macros"]
snapshot-paths: ["snapshots"]

target-path: "target" # directory which will store compiled SQL files
clean-targets: # directories to be removed by `dbt clean`
  - "target"
  - "dbt_modules"
  - "dbt_packages"

# Configuring models
# Full documentation: https://docs.getdbt.com/docs/configuring-models

# In this example config, we tell dbt to build all models in the example/ directory
# as tables. These settings can be overridden in the individual model files
# using the `{{ config(...) }}` macro.
models:
  livequery_models:
    deploy:
      core:
        +materialized: ephemeral
  datascience_models:
    gold:
      onchain_scores:
        +copy_grants: true

    streamline:
      quantum:
        poc:
          core:
            streamline__aptos_blocks_tx:
              pre-hook:
                - "SET LIVEQUERY_CONTEXT = '{\"userId\":\"aws_lambda_datascience_api\"}'"
  +copy_grants: true
  +persist_docs:
    relation: true
    columns: true
  +on_schema_change: "append_new_columns"

tests:
  +store_failures: true # all tests

on-run-end:
  - '{{ apply_meta_as_tags(results) }}'

vars:
  "dbt_date:time_zone": GMT
  UPDATE_SNOWFLAKE_TAGS: true
  UPDATE_UDFS_AND_SPS: true
  DROP_UDFS_AND_SPS: false
  START_GHA_TASKS: false
  STREAMLINE_INVOKE_STREAMS: false

  # Setup variables in dbt_project.yml
  API_INTEGRATION: '{{ var("config")[target.name]["API_INTEGRATION"] }}' 
  EXTERNAL_FUNCTION_URI: '{{ var("config")[target.name]["EXTERNAL_FUNCTION_URI"] }}'
  ROLES: '{{ var("config")[target.name]["ROLES"] }}'
  API_INTEGRATION_OPTIONS: '{{ var("config")[target.name]["API_INTEGRATION_OPTIONS"]|default({})|tojson }}'

  config:
  # The keys correspond to dbt profiles and are case sensitive
    dev:
        API_INTEGRATION: AWS_DATASCIENCE_API_STG
        EXTERNAL_FUNCTION_URI: 65sji95ax3.execute-api.us-east-1.amazonaws.com/stg/
        API_INTEGRATION_OPTIONS:
          _live.udf_api: MAX_BATCH_ROWS = 30
        ROLES:
          - INTERNAL_DEV
    prod:
      API_INTEGRATION: AWS_LIVE_QUERY
      EXTERNAL_FUNCTION_URI: bqco8lkjsb.execute-api.us-east-1.amazonaws.com/prod/
      ROLES:
        - VELOCITY_INTERNAL
        - VELOCITY_ETHEREUM
        - INTERNAL_DEV
        - BI_ANALYTICS_READER